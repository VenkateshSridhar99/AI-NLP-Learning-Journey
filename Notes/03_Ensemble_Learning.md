# ðŸŒ² Ensemble Learning (Day 4)

## What It Is:
Instead of trusting one model (like one opinion), we combine many and let them vote.

### Key Concepts:
- **Bagging** = Multiple models trained on random subsets â†’ reduces variance
- **Boosting** = Models trained one after another â†’ each tries to fix the previous one's mistakes
- **Random Forest** = Bagging + Decision Trees
- **XGBoost / Gradient Boosting** = Smart sequential learning â†’ strong performer

### Analogy:
Ask 1 doctor vs ask 100 doctors and go with majority â€” thatâ€™s ensemble learning.

## Why I Care:
This is where performance jumps happen. Random Forests + Boosting usually outperform basic ML models.
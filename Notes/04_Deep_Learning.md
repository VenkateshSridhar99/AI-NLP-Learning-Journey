# 🔍 Deep Learning (Days 5–6)

## Core Idea:
Stack layers of neurons → Let the system learn complex patterns.

### What I Covered:
- **Neurons**: Like tiny logic gates with weights
- **Activation Functions**: ReLU, Sigmoid – add non-linearity
- **Backpropagation**: Adjust weights by sending error backward
- **Vanishing Gradients**: When early layers stop learning because gradients shrink too much

### My Thought:
Deep Learning is powerful, but also easy to mess up. It’s not plug-and-play — understanding how layers and activations work matters.
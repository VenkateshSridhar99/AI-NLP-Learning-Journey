# ğŸ” Deep Learning (Days 5â€“6)

## Core Idea:
Stack layers of neurons â†’ Let the system learn complex patterns.

### What I Covered:
- **Neurons**: Like tiny logic gates with weights
- **Activation Functions**: ReLU, Sigmoid â€“ add non-linearity
- **Backpropagation**: Adjust weights by sending error backward
- **Vanishing Gradients**: When early layers stop learning because gradients shrink too much

### My Thought:
Deep Learning is powerful, but also easy to mess up. Itâ€™s not plug-and-play â€” understanding how layers and activations work matters.
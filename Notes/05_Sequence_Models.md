# ğŸ” Sequence Models (Days 7â€“8)

## Why Theyâ€™re Different:
Standard ML assumes inputs are independent.  
But language, speech, and time-series need memory. Thatâ€™s where **RNNs** come in.

### Key Models:
- **RNN**: Has memory, but forgets long-term dependencies.
- **LSTM**: Fixed that with gates and a memory cell.
- **GRU**: Simpler than LSTM but similar performance.

### Problem I Learned:
- **Vanishing Gradient**: Makes RNNs bad at remembering long-range patterns.
- Solution? LSTM/GRU.

## Application:
Now I see how chatbots, speech-to-text, and predictive text actually work. It's not magic â€” it's memory and math.
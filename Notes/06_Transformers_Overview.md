# ⚡ Transformers (Day 9)

## Why They Changed Everything:
Transformers ditched the whole idea of sequences and said:  
“Let’s look at *all* words at once.” → **Self-attention**

### Concepts:
- **Self-Attention**: Every word looks at every other word to understand context.
- **Multi-Head Attention**: Multiple perspectives at once.
- **Positional Encoding**: Since there's no sequence, this adds word order info.

### Why It’s Powerful:
- Handles long sequences better
- Fully parallel (faster than RNNs)
- Foundation of GPT, BERT, and almost all modern NLP

## My Insight:
This was the turning point in AI — where performance met scale. It’s the "deep end" of NLP.